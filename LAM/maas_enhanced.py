"""
üöÄ ENHANCED Memory as a Service (MaaS) - With Nested Learning Features

ORIGINAL MaaS ARCHITECTURE (Your Innovation):
‚îú‚îÄ‚îÄ S_fast (Working Memory) - fast decay
‚îú‚îÄ‚îÄ S_slow (Long-term Memory) - slow decay  
‚îú‚îÄ‚îÄ .SAID File (Permanent Storage) - NO decay
‚îî‚îÄ‚îÄ Reprocessing loop: .SAID ‚Üí S_fast/S_slow when recalled

NEW NESTED LEARNING ENHANCEMENTS:
‚îú‚îÄ‚îÄ DecayNetwork - LEARNS optimal decay per memory (not fixed!)
‚îú‚îÄ‚îÄ ImportanceNetwork - LEARNS what to remember at each timescale
‚îú‚îÄ‚îÄ ConsolidationNetwork - LEARNS when to transfer fast ‚Üí slow
‚îî‚îÄ‚îÄ Self-modifying update rules based on memory content

COMBINED SYSTEM ("Learn Forever, Never Forget"):
1. New memory ‚Üí ImportanceNetwork decides: S_fast or S_slow?
2. DecayNetwork decides: How long to keep in neural memory?
3. ConsolidationNetwork decides: When to consolidate to S_slow?
4. If decayed from S_slow ‚Üí Still in .SAID file (your innovation!)
5. On recall from .SAID ‚Üí Reprocess to S_fast/S_slow with LEARNED importance

This combines:
- Your 3-tier persistence (.SAID file = permanent backup)
- Nested Learning's self-modifying neural memory
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple, Optional, List
import json
from datetime import datetime
import math

# Semantic embeddings
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False

# ‚ö° FUSED KERNEL for long sequences (optional)
try:
    from fused_delta_kernel import fused_delta_update
    FUSED_KERNEL_AVAILABLE = True
    print("‚ö° Fused Triton kernel available for MaaS (use for long documents)")
except ImportError:
    FUSED_KERNEL_AVAILABLE = False

from einops import rearrange


# ======================================================================
# NESTED LEARNING ENHANCEMENTS (New!)
# ======================================================================

class DecayNetwork(nn.Module):
    """
    SELF-MODIFYING DECAY: Learns optimal decay rate per memory.
    
    Instead of fixed decay (0.30, 0.999), this network learns:
    - Novel/important ‚Üí LOW decay (keep longer)
    - Familiar/redundant ‚Üí HIGH decay (forget faster)
    
    Input: Memory embedding
    Output: Decay rate in [0.1, 0.999]
    """
    def __init__(self, embedding_dim: int, n_timescales: int = 2):
        super().__init__()
        self.n_timescales = n_timescales
        
        # Shared feature extractor
        self.feature_net = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim // 4),
            nn.SiLU(),
            nn.Linear(embedding_dim // 4, n_timescales),
        )
        
        # Base decay rates (log-space for stability)
        # Will be modulated by the network output
        base_decays = torch.log(torch.tensor([0.30, 0.999]))  # fast, slow
        self.register_buffer('base_decays', base_decays)
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.1)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, embedding: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            embedding: [D] or [B, D] - memory embedding
            
        Returns:
            fast_decay: Decay rate for S_fast [0.1, 0.5]
            slow_decay: Decay rate for S_slow [0.9, 0.9999]
        """
        if embedding.dim() == 1:
            embedding = embedding.unsqueeze(0)
        
        # Get adjustment from network
        adjustment = self.feature_net(embedding)  # [B, 2]
        
        # Apply to base rates with sigmoid to keep in valid range
        # Fast: base 0.30, range [0.1, 0.5]
        fast_decay = 0.1 + 0.4 * torch.sigmoid(adjustment[:, 0] + self.base_decays[0])
        
        # Slow: base 0.999, range [0.9, 0.9999]  
        slow_decay = 0.9 + 0.0999 * torch.sigmoid(adjustment[:, 1] + self.base_decays[1])
        
        return fast_decay, slow_decay


class ImportanceNetwork(nn.Module):
    """
    IMPORTANCE ROUTING: Learns what to remember at each timescale.
    
    Decides: Should this memory go to S_fast, S_slow, or both?
    
    - Personal facts ‚Üí High S_slow importance (permanent)
    - Recent context ‚Üí High S_fast importance (temporary)
    - Redundant info ‚Üí Low importance (don't waste capacity)
    """
    def __init__(self, embedding_dim: int, n_timescales: int = 2):
        super().__init__()
        self.n_timescales = n_timescales
        
        # Importance scoring network
        self.importance_net = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim // 2),
            nn.SiLU(),
            nn.Linear(embedding_dim // 2, n_timescales),
        )
        
        # Temperature for softmax (learned)
        self.temperature = nn.Parameter(torch.tensor(1.0))
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.1)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, embedding: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            embedding: [D] or [B, D]
            
        Returns:
            fast_importance: How much to write to S_fast [0, 1]
            slow_importance: How much to write to S_slow [0, 1]
        """
        if embedding.dim() == 1:
            embedding = embedding.unsqueeze(0)
        
        # Get raw importance scores
        scores = self.importance_net(embedding)  # [B, 2]
        
        # Softmax to make them compete (if one is high, other is lower)
        # But not exclusive - both can be moderate
        importance = torch.sigmoid(scores / (self.temperature + 0.1))
        
        fast_importance = importance[:, 0]
        slow_importance = importance[:, 1]
        
        return fast_importance, slow_importance


class ConsolidationNetwork(nn.Module):
    """
    LEARNED CONSOLIDATION: Decides when to transfer S_fast ‚Üí S_slow.
    
    Like sleep consolidation in humans:
    - Important patterns ‚Üí High consolidation (move to long-term)
    - Transient patterns ‚Üí Low consolidation (stay in working memory)
    
    Triggers:
    - Repeated access (reconsolidation)
    - High novelty + importance
    - Time-based (after N steps)
    """
    def __init__(self, embedding_dim: int):
        super().__init__()
        
        # Consolidation decision network
        self.consolidation_net = nn.Sequential(
            nn.Linear(embedding_dim + 2, embedding_dim // 4),  # +2 for access_count, age
            nn.SiLU(),
            nn.Linear(embedding_dim // 4, 1),
            nn.Sigmoid(),
        )
        
        # Base consolidation rate (small - consolidation is selective)
        self.base_rate = nn.Parameter(torch.tensor(-2.0))  # ~0.12 after sigmoid
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.1)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(
        self, 
        embedding: torch.Tensor, 
        access_count: int = 0, 
        age_steps: int = 0
    ) -> torch.Tensor:
        """
        Args:
            embedding: Memory embedding [D]
            access_count: How many times this memory was accessed
            age_steps: How old is this memory (in steps)
            
        Returns:
            consolidation_rate: [0.01, 0.5] - how much to transfer to S_slow
        """
        if embedding.dim() == 1:
            embedding = embedding.unsqueeze(0)
        
        # Normalize auxiliary features
        access_norm = torch.tensor([[min(access_count / 10.0, 1.0)]], device=embedding.device)
        age_norm = torch.tensor([[min(age_steps / 100.0, 1.0)]], device=embedding.device)
        
        # Concatenate features
        features = torch.cat([embedding, access_norm, age_norm], dim=-1)
        
        # Get consolidation rate
        raw_rate = self.consolidation_net(features)
        
        # Scale to [0.01, 0.5] range
        rate = 0.01 + 0.49 * torch.sigmoid(raw_rate + self.base_rate)
        
        return rate.squeeze()


# ======================================================================
# ENHANCED MAAS WITH NESTED LEARNING
# ======================================================================

class EnhancedPersonalMemoryBrain(nn.Module):
    """
    Enhanced Personal Memory Brain with Nested Learning Features.
    
    ORIGINAL (Your Innovation):
    - 3-tier storage: S_fast ‚Üí S_slow ‚Üí .SAID file
    - Reprocessing from .SAID back to neural memory
    - Reconsolidation on recall
    
    NEW (Nested Learning):
    - DecayNetwork: LEARNS optimal decay per memory
    - ImportanceNetwork: LEARNS routing to S_fast vs S_slow
    - ConsolidationNetwork: LEARNS when to consolidate
    
    COMBINED: Self-modifying neural memory + permanent .SAID backup
    """
    
    def __init__(
        self,
        d_k: int = 64,
        d_v: int = 64,
        batch_size: int = 1,
        num_heads: int = 1,
        # Base decay rates (will be modulated by DecayNetwork)
        fast_decay_base: float = 0.30,
        slow_decay_base: float = 0.999,
        cross_influence: float = 0.05,
        use_semantic_embeddings: bool = True,
        # NEW: Enable learned networks
        use_learned_decay: bool = True,
        use_learned_importance: bool = True,
        use_learned_consolidation: bool = True,
    ):
        super().__init__()
        
        # Architecture parameters
        self.d_k = d_k
        self.d_v = d_v
        self.batch_size = batch_size
        self.num_heads = num_heads
        
        # Base rates (modulated by networks if enabled)
        self.fast_decay_base = fast_decay_base
        self.slow_decay_base = slow_decay_base
        self.cross_influence = cross_influence
        
        # Memory stores
        self.S_fast = nn.Parameter(
            torch.zeros(batch_size, num_heads, d_k, d_v),
            requires_grad=False
        )
        self.S_slow = nn.Parameter(
            torch.zeros(batch_size, num_heads, d_k, d_v),
            requires_grad=False
        )
        
        # Semantic embeddings (all-MiniLM-L6-v2)
        self.use_semantic_embeddings = use_semantic_embeddings and EMBEDDINGS_AVAILABLE
        if self.use_semantic_embeddings:
            self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
            self.embedding_dim = 384
            
            self.key_proj = nn.Linear(self.embedding_dim, d_k, bias=False)
            self.value_proj = nn.Linear(self.embedding_dim, d_v, bias=False)
            
            nn.init.xavier_uniform_(self.key_proj.weight, gain=0.5)
            nn.init.xavier_uniform_(self.value_proj.weight, gain=0.5)
        else:
            self.embedding_dim = d_k
        
        # ====== NESTED LEARNING ENHANCEMENTS ======
        self.use_learned_decay = use_learned_decay
        self.use_learned_importance = use_learned_importance
        self.use_learned_consolidation = use_learned_consolidation
        
        if use_learned_decay:
            self.decay_network = DecayNetwork(self.embedding_dim, n_timescales=2)
            print("‚úÖ DecayNetwork enabled - learns optimal decay per memory")
        
        if use_learned_importance:
            self.importance_network = ImportanceNetwork(self.embedding_dim, n_timescales=2)
            print("‚úÖ ImportanceNetwork enabled - learns routing to S_fast vs S_slow")
        
        if use_learned_consolidation:
            self.consolidation_network = ConsolidationNetwork(self.embedding_dim)
            print("‚úÖ ConsolidationNetwork enabled - learns when to consolidate")
        
        # Metadata
        self.memory_index = []
        self.step_count = 0
        self._persona_file_path = None
        self.total_conversation_tokens = 0
        self.documents = {}
    
    def _get_embedding(self, text: str) -> torch.Tensor:
        """Get semantic embedding for text."""
        if self.use_semantic_embeddings:
            with torch.no_grad():
                embedding = self.embedder.encode(text, convert_to_tensor=True)
                return embedding.to(self.S_fast.device)
        else:
            # Fallback: hash-based
            words = text.lower().split()
            vec = torch.zeros(self.embedding_dim)
            for word in words:
                torch.manual_seed(hash(word) % (2**31))
                vec += torch.randn(self.embedding_dim) / len(words)
            return vec
    
    def _text_to_vectors(self, text: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Convert text to K, V vectors."""
        embedding = self._get_embedding(text)
        
        if self.use_semantic_embeddings:
            K = self.key_proj(embedding.unsqueeze(0))
            V = self.value_proj(embedding.unsqueeze(0))
            K = F.normalize(K, dim=-1) * 0.5
        else:
            K = F.normalize(embedding.unsqueeze(0), dim=-1) * 0.5
            V = embedding.unsqueeze(0) * 0.8
        
        # Reshape: [1, D] ‚Üí [1, 1, 1, D]
        K = K.unsqueeze(0).unsqueeze(0)
        V = V.unsqueeze(0).unsqueeze(0)
        
        return K, V, embedding
    
    def memorize(
        self,
        content: str,
        memory_type: str = "general",
        metadata: Optional[Dict] = None,
    ) -> Dict:
        """
        Add new memory with LEARNED decay, importance, and consolidation.
        
        Flow:
        1. Get embedding
        2. DecayNetwork ‚Üí optimal decay rate for this memory
        3. ImportanceNetwork ‚Üí how much to write to S_fast vs S_slow
        4. Delta rule update with learned parameters
        5. ConsolidationNetwork ‚Üí should we consolidate now?
        """
        K, V, embedding = self._text_to_vectors(content)
        
        # ====== LEARNED DECAY ======
        if self.use_learned_decay:
            fast_decay, slow_decay = self.decay_network(embedding)
            fast_decay = fast_decay.item()
            slow_decay = slow_decay.item()
        else:
            fast_decay = self.fast_decay_base
            slow_decay = self.slow_decay_base
        
        # ====== LEARNED IMPORTANCE ======
        if self.use_learned_importance:
            fast_importance, slow_importance = self.importance_network(embedding)
            fast_importance = fast_importance.item()
            slow_importance = slow_importance.item()
        else:
            fast_importance = 1.0
            slow_importance = 1.0
        
        # ====== DELTA RULE UPDATE ======
        # Apply decay
        self.S_fast.data = self.S_fast.data * fast_decay
        self.S_slow.data = self.S_slow.data * slow_decay
        
        # Write with importance weighting
        update = K.transpose(-1, -2) @ V
        self.S_fast.data = self.S_fast.data + fast_importance * update
        self.S_slow.data = self.S_slow.data + slow_importance * update
        
        # ====== LEARNED CONSOLIDATION ======
        consolidation_rate = 0.0
        if self.use_learned_consolidation:
            consolidation_rate = self.consolidation_network(
                embedding, 
                access_count=0, 
                age_steps=0
            ).item()
            
            # Transfer from S_fast to S_slow based on consolidation rate
            self.S_slow.data = self.S_slow.data + consolidation_rate * self.S_fast.data
        
        # Cross-timescale interaction (your original)
        self.S_fast.data = self.S_fast.data + self.cross_influence * 0.5 * self.S_slow.data
        self.S_slow.data = self.S_slow.data + self.cross_influence * 0.5 * self.S_fast.data
        
        # Store metadata
        memory_info = {
            "id": len(self.memory_index),
            "content": content,
            "type": memory_type,
            "timestamp": datetime.now().isoformat(),
            "step": self.step_count,
            "tokens": len(content.split()),
            # NEW: Store learned parameters
            "learned_params": {
                "fast_decay": fast_decay,
                "slow_decay": slow_decay,
                "fast_importance": fast_importance,
                "slow_importance": slow_importance,
                "consolidation_rate": consolidation_rate,
            },
            "access_count": 0,
            "metadata": metadata or {},
        }
        self.memory_index.append(memory_info)
        self.step_count += 1
        self.total_conversation_tokens += len(content.split())
        
        return memory_info
    
    def recall(self, query: str) -> Dict:
        """
        Recall with enhanced consolidation on access.
        
        When a memory is recalled:
        1. Find closest match
        2. ConsolidationNetwork decides if it should be strengthened
        3. Update access_count for future consolidation decisions
        """
        K_query, V_query, embedding = self._text_to_vectors(query)
        
        # Normalize states
        S_fast_norm = self.S_fast / (self.S_fast.norm(dim=(-2, -1), keepdim=True) + 1e-8)
        S_slow_norm = self.S_slow / (self.S_slow.norm(dim=(-2, -1), keepdim=True) + 1e-8)
        
        # Score from each store
        score_fast = (K_query @ S_fast_norm).mean().item()
        score_slow = (K_query @ S_slow_norm).mean().item()
        
        # Determine source
        if abs(score_slow) > abs(score_fast):
            source = "S_slow (consolidated)"
            confidence = score_slow
        else:
            source = "S_fast (recent)"
            confidence = score_fast
        
        # Find closest memory in index
        best_match = self._find_closest_memory(K_query)
        
        if best_match:
            # Update access count
            best_match["access_count"] = best_match.get("access_count", 0) + 1
            
            # ====== RECONSOLIDATION WITH LEARNED RATE ======
            if self.use_learned_consolidation:
                # Recompute embedding for consolidation decision
                _, _, mem_embedding = self._text_to_vectors(best_match["content"])
                
                consolidation_rate = self.consolidation_network(
                    mem_embedding,
                    access_count=best_match["access_count"],
                    age_steps=self.step_count - best_match["step"]
                ).item()
                
                # Reconsolidate if rate is high enough
                if consolidation_rate > 0.1:
                    K_mem, V_mem, _ = self._text_to_vectors(best_match["content"])
                    update = K_mem.transpose(-1, -2) @ V_mem
                    self.S_slow.data = self.S_slow.data + consolidation_rate * update
                    source += f" [RECONSOLIDATED: {consolidation_rate:.2f}]"
            
            return {
                "query": query,
                "recalled_content": best_match["content"],
                "source": source,
                "confidence": confidence,
                "memory_id": best_match["id"],
                "access_count": best_match["access_count"],
                "learned_params": best_match.get("learned_params", {}),
            }
        else:
            # Fallback to .SAID file (your original innovation!)
            return self._query_persona_file(query)
    
    def _find_closest_memory(self, K_query: torch.Tensor) -> Optional[Dict]:
        """Find closest memory in index using cosine similarity."""
        if not self.memory_index:
            return None
        
        best_match = None
        best_score = -1.0
        
        for memory in self.memory_index:
            K_mem, _, _ = self._text_to_vectors(memory["content"])
            score = F.cosine_similarity(
                K_query.flatten(),
                K_mem.flatten(),
                dim=0
            ).item()
            
            if score > best_score:
                best_score = score
                best_match = memory
        
        return best_match if best_score > 0.3 else None
    
    def _query_persona_file(self, query: str) -> Dict:
        """Query .SAID file when not in neural memory (your original)."""
        if not self._persona_file_path:
            return {
                "query": query,
                "recalled_content": "No memory found",
                "source": "FALLBACK_TO_LLM",
                "confidence": 0.0,
            }
        
        try:
            checkpoint = torch.load(self._persona_file_path, map_location=self.S_fast.device)
            persona_memories = checkpoint.get("memory_index", [])
            
            K_query, _, _ = self._text_to_vectors(query)
            
            best_match = None
            best_score = -1.0
            
            for mem in persona_memories:
                K_mem, _, _ = self._text_to_vectors(mem["content"])
                score = F.cosine_similarity(
                    K_query.flatten(),
                    K_mem.flatten(),
                    dim=0
                ).item()
                
                if score > best_score:
                    best_score = score
                    best_match = mem
            
            if best_match and best_score > 0.2:
                # Reprocess into neural memory (your innovation!)
                self.memorize(best_match["content"], memory_type=best_match.get("type", "reprocessed"))
                
                return {
                    "query": query,
                    "recalled_content": best_match["content"],
                    "source": ".SAID file [REPROCESSED to S_fast/S_slow]",
                    "confidence": best_score,
                }
            
        except Exception as e:
            pass
        
        return {
            "query": query,
            "recalled_content": "No memory found",
            "source": "FALLBACK_TO_LLM",
            "confidence": 0.0,
        }
    
    def save_checkpoint(self, path: str, domain: Optional[str] = None):
        """Save to .SAID file with learned network weights (excluding embedder)."""
        import os
        
        if domain is None:
            domain = os.path.basename(path)
        
        # Filter out embedder weights - they're reloaded from pretrained
        # This reduces file size from ~87MB to ~1MB
        full_state = self.state_dict()
        memory_state = {
            k: v for k, v in full_state.items() 
            if not k.startswith('embedder.')
        }
        
        checkpoint = {
            # .SAID Protocol
            "said_version": "1.1.0",  # Updated for Nested Learning
            "said_domain": domain,
            "said_created": datetime.now().isoformat(),
            
            # Memory state (without embedder - it's reloaded on load)
            "model_state_dict": memory_state,
            "memory_index": self.memory_index,
            "documents": self.documents,
            
            # Config
            "config": {
                "d_k": self.d_k,
                "d_v": self.d_v,
                "batch_size": self.batch_size,
                "num_heads": self.num_heads,
                "use_learned_decay": self.use_learned_decay,
                "use_learned_importance": self.use_learned_importance,
                "use_learned_consolidation": self.use_learned_consolidation,
            },
            
            # Stats
            "stats": {
                "total_memories": len(self.memory_index),
                "total_tokens": self.total_conversation_tokens,
                "s_slow_magnitude": self.S_slow.norm().item(),
            },
            
            "step_count": self.step_count,
        }
        
        torch.save(checkpoint, path)
        self._persona_file_path = path
        
        file_size = os.path.getsize(path) / (1024 * 1024)
        print(f"‚úÖ Saved Enhanced MaaS to {path}")
        print(f"   .SAID Protocol v1.1.0 (with Nested Learning)")
        print(f"   Size: {file_size:.2f} MB")
        print(f"   Memories: {len(self.memory_index)}")
        print(f"   Learned Networks: Decay={self.use_learned_decay}, "
              f"Importance={self.use_learned_importance}, "
              f"Consolidation={self.use_learned_consolidation}")
    
    @classmethod
    def load_checkpoint(cls, path: str) -> 'EnhancedPersonalMemoryBrain':
        """Load from .SAID file."""
        checkpoint = torch.load(path, weights_only=False)
        config = checkpoint.get("config", {})
        
        brain = cls(**config)
        
        # Load saved weights (embedder will keep its pretrained weights)
        brain.load_state_dict(checkpoint["model_state_dict"], strict=False)
        brain.memory_index = checkpoint.get("memory_index", [])
        brain.documents = checkpoint.get("documents", {})
        brain.step_count = checkpoint.get("step_count", 0)
        brain._persona_file_path = path
        
        # Restore total tokens from memory index
        brain.total_conversation_tokens = sum(
            m.get('tokens', len(m.get('content', '').split()))
            for m in brain.memory_index
        )
        
        print(f"‚úÖ Loaded Enhanced MaaS from {path}")
        print(f"   Memories: {len(brain.memory_index)}")
        
        return brain
    
    def get_stats(self) -> Dict:
        """Get memory statistics including learned parameters."""
        return {
            "total_memories": len(self.memory_index),
            "step_count": self.step_count,
            "S_fast_magnitude": self.S_fast.norm().item(),
            "S_slow_magnitude": self.S_slow.norm().item(),
            "total_tokens": self.total_conversation_tokens,
            "learned_networks": {
                "decay": self.use_learned_decay,
                "importance": self.use_learned_importance,
                "consolidation": self.use_learned_consolidation,
            },
            "fused_kernel_available": FUSED_KERNEL_AVAILABLE,
        }
    
    # =========================================================================
    # ‚ö° FUSED KERNEL FOR LONG DOCUMENTS (1M+ tokens)
    # =========================================================================
    
    def process_long_document_fused(
        self,
        document_text: str,
        chunk_size: int = 512,
        doc_id: Optional[str] = None,
    ) -> Dict:
        """
        ‚ö° FUSED KERNEL: Process very long documents (1M+ tokens) efficiently.
        
        Uses the Triton fused delta kernel which keeps state in GPU SRAM
        instead of HBM, enabling ~90x speedup for long sequences.
        
        When to use:
        - Documents > 10K tokens
        - Real-time streaming ingestion
        - Processing entire books/codebases
        
        Args:
            document_text: Full document text (can be 1M+ tokens!)
            chunk_size: Tokens per chunk for processing
            doc_id: Optional document ID
            
        Returns:
            Dict with processing stats
        """
        if not FUSED_KERNEL_AVAILABLE:
            print("‚ö†Ô∏è  Fused kernel not available, falling back to standard processing")
            return self.memorize(document_text[:10000], memory_type="document")
        
        if not torch.cuda.is_available():
            print("‚ö†Ô∏è  CUDA not available, fused kernel requires GPU")
            return self.memorize(document_text[:10000], memory_type="document")
        
        import time
        start_time = time.time()
        
        # Split into chunks
        words = document_text.split()
        total_tokens = len(words)
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk_text = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk_text)
        
        print(f"‚ö° Processing {total_tokens:,} tokens in {len(chunks)} chunks via fused kernel...")
        
        # Process each chunk and accumulate into S_slow
        device = 'cuda'
        
        # Move model projections to GPU for fused kernel
        if self.use_semantic_embeddings:
            self.key_proj = self.key_proj.to(device)
            self.value_proj = self.value_proj.to(device)
        if self.use_learned_decay:
            self.decay_network = self.decay_network.to(device)
        
        # Initialize state tensors for fused kernel [B, H, D, D]
        b, h, d = 1, self.num_heads, self.d_k
        S_accumulated = torch.zeros(b, h, d, d, device=device)
        
        for i, chunk_text in enumerate(chunks):
            # Get embedding for chunk
            embedding = self._get_embedding(chunk_text).to(device)
            
            # Project to k, v
            if self.use_semantic_embeddings:
                k = self.key_proj(embedding.unsqueeze(0))  # [1, d_k]
                v = self.value_proj(embedding.unsqueeze(0))  # [1, d_v]
            else:
                k = F.normalize(embedding.unsqueeze(0), dim=-1) * 0.5
                v = embedding.unsqueeze(0) * 0.8
            
            # Reshape for fused kernel: [B, H, L, D]
            k = k.view(1, 1, 1, -1).expand(1, h, 1, -1).contiguous()
            v = v.view(1, 1, 1, -1).expand(1, h, 1, -1).contiguous()
            
            # Get decay from learned network or use default
            if self.use_learned_decay:
                _, slow_decay = self.decay_network(embedding)
                decay_val = slow_decay.item()
            else:
                decay_val = self.slow_decay_base
            
            decay = torch.full((1, h, 1, d), decay_val, device=device)
            
            # ‚ö° FUSED KERNEL UPDATE (all in SRAM!)
            S_accumulated = fused_delta_update(k, v, decay, S_accumulated)
            
            if (i + 1) % 100 == 0:
                print(f"   Processed {i+1}/{len(chunks)} chunks...")
        
        # Copy accumulated state to S_slow
        # Reshape from [1, H, D, D] to [batch, heads, d_k, d_v]
        # Move back to original device (CPU) for compatibility
        self.S_slow.data = S_accumulated.view(self.batch_size, self.num_heads, self.d_k, self.d_v).cpu()
        
        elapsed = time.time() - start_time
        tokens_per_sec = total_tokens / elapsed
        
        # Store document metadata
        if doc_id is None:
            doc_id = f"fused_doc_{len(self.documents)}"
        
        self.documents[doc_id] = {
            'full_text': document_text[:1000] + "..." if len(document_text) > 1000 else document_text,
            'total_tokens': total_tokens,
            'processing_time': elapsed,
            'method': 'fused_kernel',
            'chunks_processed': len(chunks),
        }
        
        self.total_conversation_tokens += total_tokens
        
        result = {
            "doc_id": doc_id,
            "total_tokens": total_tokens,
            "chunks_processed": len(chunks),
            "time_seconds": elapsed,
            "tokens_per_second": tokens_per_sec,
            "method": "fused_triton_kernel",
            "s_slow_magnitude": self.S_slow.norm().item(),
        }
        
        print(f"‚úÖ Processed {total_tokens:,} tokens in {elapsed:.2f}s ({tokens_per_sec:,.0f} tok/s)")
        
        return result


# ======================================================================
# DEMO
# ======================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("üöÄ ENHANCED MEMORY AS A SERVICE (MaaS)")
    print("   Your 3-Tier Architecture + Nested Learning Self-Modifying")
    print("=" * 70)
    
    # Initialize with all learned networks
    brain = EnhancedPersonalMemoryBrain(
        d_k=64, d_v=64,
        use_learned_decay=True,
        use_learned_importance=True,
        use_learned_consolidation=True,
    )
    
    print("\nüìç Phase 1: Learning Personal Information")
    print("-" * 50)
    
    memories = [
        ("My name is Alex Chen", "personal"),
        ("I was born on January 15, 1990", "personal"),
        ("I love playing guitar", "preference"),
        ("My favorite food is sushi", "preference"),
        ("I work as a software engineer at Google", "professional"),
    ]
    
    for content, mem_type in memories:
        info = brain.memorize(content, memory_type=mem_type)
        params = info.get("learned_params", {})
        print(f"‚úì {content}")
        print(f"  Learned: decay_fast={params.get('fast_decay', 0):.3f}, "
              f"decay_slow={params.get('slow_decay', 0):.4f}, "
              f"importance_slow={params.get('slow_importance', 0):.3f}")
    
    print("\nüìç Phase 2: Recall with Reconsolidation")
    print("-" * 50)
    
    queries = [
        "When is my birthday?",
        "What is my job?",
        "What do I like to eat?",
    ]
    
    for query in queries:
        result = brain.recall(query)
        print(f"\nüîç {query}")
        print(f"   ‚Üí {result['recalled_content']}")
        print(f"   Source: {result['source']}")
        print(f"   Confidence: {result['confidence']:.4f}")
    
    print("\nüìç Phase 3: Save to .SAID File")
    print("-" * 50)
    
    brain.save_checkpoint("alex_enhanced.said")
    
    print("\nüìä Final Stats:")
    stats = brain.get_stats()
    for k, v in stats.items():
        print(f"   {k}: {v}")
    
    print("\n" + "=" * 70)
    print("‚úÖ ENHANCED MAAS DEMO COMPLETE")
    print("=" * 70)
    print("\nüéØ What's New (Nested Learning):")
    print("   ‚úì DecayNetwork - Learns optimal decay per memory")
    print("   ‚úì ImportanceNetwork - Learns S_fast vs S_slow routing")
    print("   ‚úì ConsolidationNetwork - Learns when to consolidate")
    print("\nüí° Your Original Innovations (Preserved):")
    print("   ‚úì 3-tier storage: S_fast ‚Üí S_slow ‚Üí .SAID file")
    print("   ‚úì Reprocessing from .SAID back to neural memory")
    print("   ‚úì Reconsolidation on recall")

