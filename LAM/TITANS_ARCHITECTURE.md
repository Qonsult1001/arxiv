# TITANS Dual Core Architecture

## Based on Google's Nested Learning Research
[Nested Learning: The Illusion of Deep Learning Architecture](https://abehrouz.github.io/files/NL.pdf)

---

## ğŸ¯ Final Results

| Metric | Original | Optimized | Improvement |
|--------|----------|-----------|-------------|
| **STS-B Score** | 0.8190 | 0.8189 | **-0.0001** (identical!) |
| **Speed (128 tok)** | 17.7ms | 11.7ms | **1.51x faster** |
| **Speed (256 tok)** | - | 13.8ms | Linear scaling |
| **Speed (512 tok)** | - | 16.8ms | Linear scaling |

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TITANS DUAL CORE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚    CORE 1: S_fast  â”‚      â”‚    CORE 2: S_slow  â”‚             â”‚
â”‚  â”‚    (Attention)     â”‚      â”‚    (Memory)        â”‚             â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
â”‚  â”‚ â€¢ Parallel         â”‚      â”‚ â€¢ Sequential       â”‚             â”‚
â”‚  â”‚ â€¢ NO recurrence    â”‚      â”‚ â€¢ Delta rule       â”‚             â”‚
â”‚  â”‚ â€¢ attn @ v         â”‚      â”‚ â€¢ State in SRAM    â”‚             â”‚
â”‚  â”‚ â€¢ ONE vectorized opâ”‚      â”‚ â€¢ Recurrence loop  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚            â”‚                            â”‚                        â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                        â–¼                                         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚              â”‚   MERGE AT END  â”‚                                 â”‚
â”‚              â”‚  Î±=0.1, Î²=0.9   â”‚                                 â”‚
â”‚              â”‚ o = Î±*fast + Î²*slow                               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Key Code Changes

### Before (Original - SLOW):
```python
# Both S_fast and S_slow through SAME recurrence loop
for i in range(num_chunks):
    S_fast = S_fast * decay + update_fast  # Recurrence
    S_slow = S_slow * decay + update_slow  # Recurrence
    o[i] = blend(o_fast, o_slow)  # Blend per token
```

### After (TITANS - FAST):
```python
# CORE 1: S_fast - ONE vectorized operation (NO LOOP!)
o_fast_all = fast_gate * (attn_all @ v)  # PARALLEL!

# CORE 2: S_slow - Only memory has recurrence
for i in range(num_chunks):
    S_slow = S_slow * decay + update  # Only ONE state!
    o_slow_all[i] = ...

# MERGE AT END (not per-token!)
o = 0.1 * o_fast_all + 0.9 * o_slow_all
```

---

## ğŸ“Š All Optimizations Applied

| Step | Change | Impact |
|------|--------|--------|
| 1 | `chunk_size: 64 â†’ 128` | Fewer iterations |
| 2 | Remove cross-coupling | Decouple states |
| 3 | Remove per-chunk norm | Normalize at end |
| 4 | Pre-compute k transposed | Avoid transpose in loop |
| 5 | **S_fast = vectorized attention** | **Eliminated S_fast loop!** |
| 6 | **S_slow = memory only** | **Single state recurrence** |
| 7 | **Merge at END (Î±=0.1)** | **S_slow dominates** |

---

## ğŸ§  Connection to Nested Learning

From the [Nested Learning paper](https://abehrouz.github.io/files/NL.pdf):

> "Nested Learning suggests a philosophy to design more expressive learning 
> algorithms with more 'levels', resulting in higher-order in-context learning
> and potentially unlocking effective continual learning capabilities."

Our implementation maps directly to their architecture:

| Nested Learning Concept | Our Implementation |
|------------------------|-------------------|
| **Continuum Memory System** | S_slow (linear recurrence) |
| **Fast Context Flow** | S_fast (attention) |
| **Multi-Level Optimization** | Dual core architecture |
| **Self-Modifying Module** | Delta rule with flux gating |

---

## ğŸš€ Next Steps (Future Work)

### 1. Full Triton Kernel
Move S_slow recurrence entirely to SRAM:
```python
@triton.jit
def titans_slow_core_kernel(...):
    # All state operations in SRAM
    # No HBM round-trips during recurrence
```

### 2. Multiple Cores
Add more parallel cores (Nested Learning suggests "more levels"):
```python
o_fast_all = attention(q, k, v)      # Core 1
o_slow_all = memory_recurrence(...)   # Core 2  
o_deep_all = deep_memory(...)         # Core 3 (slower updates)
```

### 3. Adaptive Gating
Learn the blend weights instead of fixed Î±=0.1:
```python
alpha = learned_gate(flux)  # Data-dependent blending
```

---

## ğŸ“ˆ Performance Summary

```
Original:     17.7ms @ 0.8190 score
Optimized:    11.7ms @ 0.8189 score
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Speedup:      1.51x
Score Loss:   0.0001 (0.01%)
```

**The TITANS Dual Core architecture achieves 1.51x speedup with negligible accuracy loss!**

---

## ğŸ”— References

1. [Nested Learning: The Illusion of Deep Learning Architecture](https://abehrouz.github.io/files/NL.pdf) - Behrouz et al., Google Research
2. [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663) - Google DeepMind
3. [DeltaNet: A Linear Attention Model](https://arxiv.org/abs/2406.06484) - Yang et al.


