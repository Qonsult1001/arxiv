# ğŸ” ROOT CAUSE ANALYSIS: Retrieval Score Gap

## **Problem**
- **Baseline (all-MiniLM-L6-v2)**: 64.51 on SciFact
- **LAM**: 40.08 on SciFact  
- **Gap**: -24.43 points âš ï¸

## **Investigation Results**

### âœ… **What We Verified (Working Correctly)**
1. **Embedding Normalization**: âœ… All embeddings are L2-normalized (norms = 1.0)
2. **Similarity Computation**: âœ… Cosine similarity works correctly
3. **Code Paths**: âœ… Both fast path and streaming path work
4. **Speed**: âœ… LAM is actually FASTER (0.040s vs 0.214s)
5. **Method Signatures**: âœ… All required methods exist

### âš ï¸ **Key Finding: Embeddings Are Different**
- **Cosine similarity between baseline and LAM**: 0.44-0.58 (very different!)
- **This is EXPECTED** - LAM has 6 trained DeltaNet layers that transform embeddings
- **Semantic quality**: LAM embeddings are semantically reasonable (finds correct docs)

### ğŸ” **ROOT CAUSE: Training Data Mismatch**

**LAM was trained on:**
- âœ… **AllNLI** (sentence pairs for semantic similarity)
- âœ… **STS-B** (semantic textual similarity)
- âœ… **QA pairs** (question-answer pairs)

**LAM was NOT trained on:**
- âŒ **Retrieval tasks** (SciFact, NFCorpus, etc.)
- âŒ **Hard negative mining** (triplet loss)
- âŒ **Retrieval-specific ranking**

### ğŸ“Š **Why This Explains the Gap**

1. **STS Performance**: âœ… **81.88** (target: 81.0) - **PASS**
   - Model was trained for semantic similarity
   - DeltaNet layers optimized for STS tasks

2. **Retrieval Performance**: âŒ **40.08** (baseline: 64.51) - **FAIL**
   - Model was NOT trained for retrieval ranking
   - DeltaNet layers optimized for similarity, not ranking
   - Retrieval requires different embedding space structure

### ğŸ’¡ **The Confusion**

**User's expectation**: "We use the same frozen embeddings and FFNs, so scores should be similar"

**Reality**: 
- âœ… Same frozen embeddings (base layer)
- âœ… Same frozen FFNs (base layer)
- âŒ **BUT**: 6 trained DeltaNet layers transform embeddings
- âŒ These layers are optimized for STS, not retrieval

### ğŸ”§ **Solution**

**Option 1: Fine-tune on Retrieval Tasks** (Recommended)
- Fine-tune LAM on retrieval datasets (SciFact, NFCorpus, etc.)
- Use triplet loss with hard negative mining
- Optimize for NDCG@10 metric (retrieval ranking)

**Option 2: Use Base Model for Retrieval** (Not recommended)
- Skip DeltaNet layers for retrieval
- Use only frozen embeddings + FFNs
- Would match baseline but lose LAM's benefits

**Option 3: Multi-task Training** (Future)
- Train on both STS and retrieval tasks simultaneously
- Balance similarity and ranking objectives

---

## **Conclusion**

**This is NOT a code bug** - the code is working correctly.

**This IS a training data issue** - the model needs retrieval-specific fine-tuning.

The 24-point gap is because:
1. Model was trained for semantic similarity (STS)
2. Retrieval requires different embedding space structure
3. DeltaNet layers optimized for similarity, not ranking

**Next Steps**: Fine-tune LAM on retrieval tasks to close the gap.


