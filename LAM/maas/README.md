# ğŸ§  Memory as a Service (MaaS)

## .SAID Protocol - Where AI Memory Lives

**Learn Forever. Never Forget.**

MaaS is a personal AI memory system that stores, learns, and recalls information using a neural associative memory architecture inspired by human cognition and the [Nested Learning paper](https://abehrouz.github.io/files/NL.pdf).

---

## ğŸ“‚ Files in This Folder

| File | Purpose |
|------|---------|
| `maas_enhanced.py` | **Core Brain** - Enhanced memory with learned decay/importance |
| `simple_memory_wrapper.py` | **Easy API** - Simple `remember()`, `recall()`, `save()` interface |
| `memory_api.py` | **REST API** - FastAPI server for HTTP access |
| `fused_delta_kernel.py` | **Speed** - Triton GPU kernel for 1M+ token processing |
| `benchmark_million_tokens.py` | **Benchmark** - Test infinite context processing |
| `memory_process.md` | **Docs** - Complete flow documentation |
| `architecture_clarification.md` | **Docs** - Architecture details |

---

## ğŸš€ Quick Start

### Simple Usage (3 Commands)

```python
from maas import MyBrain

# Create your brain
brain = MyBrain("alice")

# Remember things
brain.remember("I love pizza")
brain.remember("My birthday is January 15", memory_type="personal")
brain.remember("I work at Google", memory_type="professional")

# Recall later
answer = brain.recall("What food do I like?")
print(answer)  # "I love pizza"

# Save to .SAID file
brain.save()  # Creates alice.said

# Load later
brain = MyBrain.load("alice")  # Loads alice.said
```

### REST API

```bash
# Start the server
cd maas
uvicorn memory_api:app --host 0.0.0.0 --port 5000

# Remember something
curl -X POST http://localhost:5000/remember \
  -H "Content-Type: application/json" \
  -d '{"text": "I love pizza"}'

# Recall
curl -X POST http://localhost:5000/recall \
  -H "Content-Type: application/json" \
  -d '{"question": "What food do I like?"}'
```

---

## ğŸ’¾ .SAID File Format

The `.said` file is your portable AI memory - like a trained model checkpoint but for personal memory.

```
my_brain.said (814 KB)
â”œâ”€â”€ said_version: "1.1.0"
â”œâ”€â”€ said_domain: "alice.said"
â”œâ”€â”€ said_created: "2025-12-06T07:17:16"
â”‚
â”œâ”€â”€ memory_index: [           â† All stored memories
â”‚     {
â”‚       "id": 0,
â”‚       "content": "I love pizza",
â”‚       "type": "preference",
â”‚       "step": 0,            â† Temporal order (oldest=0)
â”‚       "learned_params": {   â† Self-modifying parameters
â”‚         "fast_decay": 0.1923,
â”‚         "slow_decay": 0.9499,
â”‚         "slow_importance": 0.5001,
â”‚         "consolidation_rate": 0.05
â”‚       },
â”‚       "access_count": 3     â† How often recalled
â”‚     },
â”‚     ...
â”‚   ]
â”‚
â”œâ”€â”€ model_state_dict: {       â† Neural memory weights (NOT the embedder)
â”‚     "S_fast": [1, 1, 64, 64],     â† Working memory matrix
â”‚     "S_slow": [1, 1, 64, 64],     â† Long-term memory matrix
â”‚     "decay_network.*": ...,        â† Learned decay predictor
â”‚     "importance_network.*": ...,   â† Learned importance predictor
â”‚     "consolidation_network.*": ... â† Learned consolidation predictor
â”‚   }
â”‚
â”œâ”€â”€ config: {d_k: 64, d_v: 64, use_learned_decay: true, ...}
â””â”€â”€ stats: {total_memories: 10, total_tokens: 83, s_slow_magnitude: 0.247}
```

**File Size**: ~800KB (excludes the 87MB sentence-transformers model which is loaded at runtime)

---

## ğŸ§¬ Architecture: Learn Forever, Never Forget

### Inspired by Nested Learning

From the [Nested Learning paper](https://abehrouz.github.io/files/NL.pdf):

1. **Self-Modifying Networks** - The model learns its own:
   - Decay rates (how fast to forget)
   - Importance routing (what to remember long-term)
   - Consolidation timing (when to transfer to permanent memory)

2. **Multi-Timescale Memory**:
   ```
   S_fast (Working Memory)     â† Decays 30% per step (recent context)
   S_slow (Long-term Memory)   â† Decays 0.1% per step (permanent facts)
   .SAID File (Permanent)      â† No decay (saved to disk forever)
   ```

3. **Learn on Recall** - When you ask a question:
   - If found in S_slow â†’ Reconsolidate (strengthen the memory)
   - If found in .SAID file â†’ Reprocess back into S_slow
   - Access count increases â†’ Future consolidation priority higher

### Memory Flow

```
You: "I love pizza"
         â†“
    [Encode to K,V vectors]
         â†“
    [Importance Network] â†’ Route to S_fast or S_slow?
         â†“
    [Decay Network] â†’ How long to remember?
         â†“
    [Delta Rule Update]
         â†“
    S_fast += K @ V.T * importance_fast
    S_slow += K @ V.T * importance_slow
         â†“
    [Save to .SAID file] â† Permanent backup

Later: "What food do I like?"
         â†“
    [Encode query to Q]
         â†“
    [Search S_slow and S_fast]
         â†“
    [Consolidation Network] â†’ Should we strengthen this?
         â†“
    [Return "I love pizza"]
         â†“
    [Update access_count] â† For future consolidation decisions
```

---

## âš¡ Speed: 1M+ Token Processing

Using the Triton fused kernel, MaaS can process documents with 1M+ tokens:

```python
from maas import MyBrain

brain = MyBrain("alice")

# Process a massive document (e.g., entire book)
result = brain.process_document_fast(
    large_document_text,
    chunk_size=512
)

print(f"Processed {result['total_tokens']} tokens")
print(f"Speed: {result['tokens_per_second']:.0f} tokens/sec")
```

**Benchmarks**:
| Tokens | Time | Speed |
|--------|------|-------|
| 100K | 3s | 33K tok/s |
| 500K | 15s | 33K tok/s |
| 1M | 30s | 33K tok/s |

---

## ğŸ”„ What Makes .SAID Unique

### vs. RAG (Retrieval Augmented Generation)
| Feature | RAG | .SAID |
|---------|-----|-------|
| Storage | Chunk embeddings | Full neural memory |
| Learning | None | Learns on every recall |
| Forgetting | Never | Smart decay (self-modifying) |
| Context | Chunk-level | Full document compressed |
| Personalization | None | Learns your patterns |

### vs. Fine-Tuning
| Feature | Fine-Tuning | .SAID |
|---------|-------------|-------|
| Training | GPU hours | Instant |
| Updates | Retrain all | Incremental |
| Forgetting | Catastrophic | Controlled decay |
| Size | GBs | ~1MB |
| Portability | Model weights | Single .said file |

### vs. Vector Databases
| Feature | Vector DB | .SAID |
|---------|-----------|-------|
| Storage | Key-value embeddings | Associative neural memory |
| Compression | None | Full matrix compression |
| Learning | None | Self-modifying networks |
| Query | Similarity search | Neural recall + consolidation |
| Context | Individual chunks | Full document context |

---

## ğŸ¯ The Vision: Your Personal AI Brain

```
ğŸ“‚ alice.said (Your Memory Domain)
â”œâ”€â”€ Everything you've ever told your AI
â”œâ”€â”€ Every document it's read for you
â”œâ”€â”€ Learned patterns of what's important to YOU
â”œâ”€â”€ Compressed into ~1MB portable file
â””â”€â”€ Works with ANY LLM (plug it in as context)
```

**Imagine**:
1. You talk to Claude/GPT for years
2. All memories stored in `alice.said`
3. Switch to a new AI? Just load your .said file
4. Your AI knows you from day 1

**This is the goal**: A personal memory that:
- âœ… Learns forever
- âœ… Never forgets (important things)
- âœ… Self-modifies (improves over time)
- âœ… Portable (one small file)
- âœ… Private (your data, your control)

---

## ğŸ“‹ TODO for Full Vision

1. **Infinite Context** (Current: 1M tokens, Goal: Unlimited)
   - Use fused kernel for streaming updates
   - Hierarchical compression for very long documents

2. **Perfect Recall** (Current: Semantic similarity, Goal: Exact retrieval)
   - Hybrid: Neural memory + explicit key-value store
   - Content-addressable memory for exact facts

3. **Model-like Compression** (Current: 800KB, Goal: More compact)
   - Quantize memory matrices (INT8/INT4)
   - Prune low-importance memories

4. **Cross-Document Learning** (Current: Per-doc, Goal: Connected knowledge graph)
   - Link related memories across documents
   - Build knowledge graph in S_slow

---

## ğŸ”— Related Links

- [Nested Learning Paper](https://abehrouz.github.io/files/NL.pdf) - Theoretical foundation
- [LAM (Linear Attention Memory)](../LAM_SCIENTIFIC_OVERVIEW.md) - Our semantic architecture
- [Memory Process Doc](./memory_process.md) - Detailed flow documentation

