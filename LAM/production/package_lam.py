"""
LAM Model Packaging Script

Creates a distribution package containing:
- Base model (pytorch_model.bin from all-MiniLM-L6-v2)
- LAM checkpoint (checkpoint_best_3500.pt)
- Tokenizer files
- Configuration
- Loading wrapper

Usage:
    python production/package_lam.py
"""

import shutil
import json
from pathlib import Path
import tarfile


def package_lam_model(
    output_dir: str = "production/lam-base-v1",
    create_tarball: bool = True
):
    """
    Package LAM model for distribution

    Args:
        output_dir: Output directory for packaged model
        create_tarball: Create .tar.gz archive
    """
    print("="*80)
    print("LAM MODEL PACKAGING")
    print("="*80)

    # Paths
    repo_root = Path(__file__).parent.parent
    output_path = repo_root / output_dir
    output_path.mkdir(parents=True, exist_ok=True)

    # Source paths
    base_model_dir = repo_root / 'all-MiniLM-L6-v2'
    checkpoint_path = repo_root / 'proper_distillation_reaccelerate' / 'checkpoint_best_3500.pt'

    print(f"\nüì¶ Packaging LAM model to: {output_path}")

    # 1. Copy base model files
    print(f"\n1Ô∏è‚É£  Copying base model files from {base_model_dir}...")

    files_to_copy = [
        'pytorch_model.bin',
        'config.json',
        'vocab.txt',
        'tokenizer.json',
        'tokenizer_config.json',
        'special_tokens_map.json',
    ]

    for filename in files_to_copy:
        src = base_model_dir / filename
        dst = output_path / filename

        if src.exists():
            shutil.copy2(src, dst)
            size_mb = src.stat().st_size / (1024 * 1024)
            print(f"   ‚úÖ {filename} ({size_mb:.1f} MB)")
        else:
            print(f"   ‚ö†Ô∏è  {filename} not found")

    # 2. Copy LAM checkpoint
    print(f"\n2Ô∏è‚É£  Copying LAM checkpoint...")

    if checkpoint_path.exists():
        dst = output_path / 'lam_checkpoint.pt'
        shutil.copy2(checkpoint_path, dst)
        size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
        print(f"   ‚úÖ lam_checkpoint.pt ({size_mb:.1f} MB)")
        print(f"   üìä Performance: 0.836 Pearson on STS-B")
    else:
        print(f"   ‚ùå Checkpoint not found at {checkpoint_path}")
        return

    # 3. Create LAM-specific config
    print(f"\n3Ô∏è‚É£  Creating LAM configuration...")

    lam_config = {
        "model_type": "lam",
        "architecture": "Enhanced Hierarchical DeltaNet",
        "base_model": "all-MiniLM-L6-v2",
        "d_model": 384,
        "num_heads": 12,
        "num_layers": 6,
        "fast_decay_init": 0.3,
        "slow_decay_init": 0.85,
        "max_seq_length": 128,
        "performance": {
            "stsb_pearson": 0.836,
            "stsb_spearman": 0.832,
            "parameters": "22M",
            "complexity": "O(n)",
            "max_context": "1M+ tokens"
        },
        "training": {
            "stage_1": "AllMiniLM distillation (50B tokens)",
            "stage_2": "E5-Large distillation (30B tokens)",
            "total_tokens": "80B"
        },
        "version": "1.0.0",
        "license": "Proprietary Commercial License"
    }

    with open(output_path / 'lam_config.json', 'w') as f:
        json.dump(lam_config, f, indent=2)
    print(f"   ‚úÖ lam_config.json")

    # 4. Create README
    print(f"\n4Ô∏è‚É£  Creating README...")

    readme_content = """# LAM Base v1

**Linear Attention Model achieving 0.836 Pearson on STS-B**

## Quick Start

```python
from lam_wrapper import LAMEncoder

# Load model
model = LAMEncoder.from_pretrained('lam-base-v1')

# Encode sentences
embeddings = model.encode([
    "This is a sentence",
    "This is another sentence"
])

# Compute similarity
from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity(embeddings)
print(f"Similarity: {similarity[0][1]:.4f}")
```

## Performance

| Metric | Value |
|--------|-------|
| STS-B Pearson | 0.836 |
| STS-B Spearman | 0.832 |
| Model Size | ~110 MB |
| Parameters | 22M |
| Dimensions | 384 |
| Complexity | O(n) linear |
| Max Context | 1M+ tokens |

## Architecture

LAM uses a hybrid architecture:
- **Base embeddings**: From all-MiniLM-L6-v2 (frozen)
- **Attention layers**: 6 Enhanced Hierarchical DeltaNet layers (O(n) complexity)
- **FFN layers**: From all-MiniLM-L6-v2
- **Pooling**: Mean pooling + L2 normalization

## Files

- `pytorch_model.bin` - Base model weights (~90 MB)
- `lam_checkpoint.pt` - LAM DeltaNet weights (~18 MB)
- `config.json` - Model configuration
- `lam_config.json` - LAM-specific configuration
- `vocab.txt` - Vocabulary (30,522 tokens)
- `tokenizer.json` - Tokenizer configuration
- `tokenizer_config.json` - Tokenizer settings
- `special_tokens_map.json` - Special tokens

## License

**Proprietary Commercial License**

LAM model is proprietary. Contact for licensing information.

Base model (all-MiniLM-L6-v2) is Apache 2.0 licensed.

## Citation

If you use LAM in your research or application, please cite:

```bibtex
@misc{lam2025,
  title={LAM: Linear Attention Model with Hierarchical Memory},
  year={2025},
  note={First linear attention model achieving 0.836 Pearson on STS-B}
}
```

## Support

For support, licensing, or questions, contact: [Your Contact]
"""

    with open(output_path / 'README.md', 'w') as f:
        f.write(readme_content)
    print(f"   ‚úÖ README.md")

    # 5. Copy wrapper script
    print(f"\n5Ô∏è‚É£  Copying wrapper script...")
    wrapper_src = Path(__file__).parent / 'lam_wrapper.py'
    if wrapper_src.exists():
        shutil.copy2(wrapper_src, output_path / 'lam_wrapper.py')
        print(f"   ‚úÖ lam_wrapper.py")
    else:
        print(f"   ‚ö†Ô∏è  lam_wrapper.py not found")

    # 6. Calculate total size
    print(f"\n6Ô∏è‚É£  Calculating package size...")
    total_size = sum(f.stat().st_size for f in output_path.rglob('*') if f.is_file())
    total_size_mb = total_size / (1024 * 1024)
    print(f"   üì¶ Total package size: {total_size_mb:.1f} MB")

    # 7. Create tarball
    if create_tarball:
        print(f"\n7Ô∏è‚É£  Creating distribution archive...")
        tarball_name = f"{output_path.name}-dist.tar.gz"
        tarball_path = output_path.parent / tarball_name

        with tarfile.open(tarball_path, "w:gz") as tar:
            tar.add(output_path, arcname=output_path.name)

        tarball_size_mb = tarball_path.stat().st_size / (1024 * 1024)
        print(f"   ‚úÖ {tarball_name} ({tarball_size_mb:.1f} MB)")
        print(f"   üìÅ Location: {tarball_path}")

    # Summary
    print("\n" + "="*80)
    print("‚úÖ PACKAGING COMPLETE")
    print("="*80)
    print(f"\nüì¶ Package location: {output_path}")
    print(f"üìä Package size: {total_size_mb:.1f} MB")

    if create_tarball:
        print(f"üìÅ Distribution archive: {tarball_path}")

    print(f"\nüöÄ To use LAM in your SDK/API:")
    print(f"""
    from lam_wrapper import LAMEncoder

    model = LAMEncoder.from_pretrained('{output_path}')
    embeddings = model.encode(["Your text here"])
    """)

    print(f"\nüîí Security Notes:")
    print(f"   - Core formula (final_solution_formula.py) is NOT included")
    print(f"   - Only trained weights and inference wrapper distributed")
    print(f"   - Proprietary commercial license applies")

    return output_path


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Package LAM model for distribution")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="production/lam-base-v1",
        help="Output directory for packaged model"
    )
    parser.add_argument(
        "--no-tarball",
        action="store_true",
        help="Skip creating .tar.gz archive"
    )

    args = parser.parse_args()

    package_lam_model(
        output_dir=args.output_dir,
        create_tarball=not args.no_tarball
    )
